{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from mmengine import Config\n",
    "from functools import partial\n",
    "\n",
    "# if you want \n",
    "from mmrotate.registry import MODELS\n",
    "from mmrotate.utils import register_all_modules\n",
    "register_all_modules()\n",
    "\n",
    "from mmdet.registry import MODELS\n",
    "from mmdet.utils import register_all_modules\n",
    "register_all_modules()\n",
    "\n",
    "\n",
    "from mmengine.runner import Runner\n",
    "from torchview import draw_graph\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "0 {'inputs': [tensor([[[115, 115, 115,  ..., 199, 193, 189],\n",
      "         [115, 115, 116,  ..., 207, 203, 199],\n",
      "         [115, 116, 117,  ..., 213, 212, 210],\n",
      "         ...,\n",
      "         [  4,   6,  11,  ..., 145, 108,  64],\n",
      "         [  7,   6,  11,  ...,  65,  50,  39],\n",
      "         [  5,   7,   8,  ...,  33,  27,  22]],\n",
      "\n",
      "        [[ 21,  21,  22,  ..., 175, 172, 170],\n",
      "         [ 21,  22,  23,  ..., 176, 173, 172],\n",
      "         [ 22,  23,  24,  ..., 178, 176, 174],\n",
      "         ...,\n",
      "         [  0,   4,  10,  ..., 108,  75,  34],\n",
      "         [  0,   2,   8,  ...,  35,  21,  11],\n",
      "         [  0,   1,   5,  ...,   7,   3,   0]],\n",
      "\n",
      "        [[  2,   2,   1,  ..., 143, 137, 133],\n",
      "         [  2,   1,   2,  ..., 143, 138, 135],\n",
      "         [  1,   2,   3,  ..., 144, 140, 136],\n",
      "         ...,\n",
      "         [  5,  10,  19,  ...,  85,  46,   8],\n",
      "         [  7,   8,  15,  ...,  19,  11,   4],\n",
      "         [  5,   8,  11,  ...,   2,  11,  14]]], dtype=torch.uint8)], 'data_samples': [<DetDataSample(\n",
      "\n",
      "    META INFORMATION\n",
      "    img_path: 'data/coco/val2017/000000000009.jpg'\n",
      "    img_shape: (240, 320)\n",
      "    img_id: 0\n",
      "    ori_shape: (480, 640)\n",
      "    scale_factor: (0.5, 0.5)\n",
      "\n",
      "    DATA FIELDS\n",
      "    gt_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([[  1., 187., 612., 472.],\n",
      "                        [311.,   4., 630., 232.],\n",
      "                        [249., 229., 565., 474.],\n",
      "                        [  0.,  13., 434., 388.],\n",
      "                        [376.,  40., 451.,  86.],\n",
      "                        [465.,  38., 523.,  84.],\n",
      "                        [385.,  73., 469., 143.],\n",
      "                        [364.,   2., 458.,  73.]]))\n",
      "            labels: tensor([45, 45, 50, 45, 49, 49, 49, 49])\n",
      "        ) at 0x7f35e2896130>\n",
      "    ignored_instances: <InstanceData(\n",
      "        \n",
      "            META INFORMATION\n",
      "        \n",
      "            DATA FIELDS\n",
      "            bboxes: HorizontalBoxes(\n",
      "                tensor([], size=(0, 4)))\n",
      "            labels: tensor([], dtype=torch.int64)\n",
      "        ) at 0x7f35e2896f10>\n",
      ") at 0x7f35e28963a0>]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_graph.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = '../mmdetection/configs/yolo/yolov3_d53_8xb8-320-273e_coco.py'\n",
    "\n",
    "cfg = Config.fromfile(config)\n",
    "\n",
    "dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "\n",
    "for idx, data_batch in enumerate(dataloader):\n",
    "    print(idx, data_batch)\n",
    "    break\n",
    "\n",
    "model = MODELS.build(cfg.model)\n",
    "_forward = model.forward\n",
    "\n",
    "data = model.data_preprocessor(data_batch)\n",
    "model.forward = partial(_forward, data_samples=data['data_samples'])\n",
    "\n",
    "\n",
    "summary(model, data['inputs'].shape, depth=3)\n",
    "# summary(model, (1, 3, 1024, 1024), depth=3)\n",
    "model_graph = draw_graph(model, input_size=data['inputs'].shape)\n",
    "model_graph.visual_graph\n",
    "\n",
    "model_graph.visual_graph.render(filename='model_graph', view=False, cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmroate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee37251af7087ac60bc1214ce22d8f37d8d56e9e600386add225441fca80f03b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
